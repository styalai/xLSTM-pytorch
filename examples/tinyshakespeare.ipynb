{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-04T10:30:51.285190Z",
     "iopub.status.busy": "2024-06-04T10:30:51.284834Z",
     "iopub.status.idle": "2024-06-04T10:30:51.671159Z",
     "shell.execute_reply": "2024-06-04T10:30:51.670250Z",
     "shell.execute_reply.started": "2024-06-04T10:30:51.285160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T10:30:52.865778Z",
     "iopub.status.busy": "2024-06-04T10:30:52.865150Z",
     "iopub.status.idle": "2024-06-04T10:31:08.850875Z",
     "shell.execute_reply": "2024-06-04T10:31:08.849977Z",
     "shell.execute_reply.started": "2024-06-04T10:30:52.865745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Libric0/xLSTM-pytorch\n",
      "  Cloning https://github.com/Libric0/xLSTM-pytorch to /tmp/pip-req-build-ymotmx0h\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Libric0/xLSTM-pytorch /tmp/pip-req-build-ymotmx0h\n",
      "  Resolved https://github.com/Libric0/xLSTM-pytorch to commit 82ff081eeb42cf5a1c0f54600e1442cd77afee18\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#!pip install git+https://github.com/styalai/Mamba-pytorch\n",
    "#!pip install git+https://github.com/ZiyaoLi/fast-kan\n",
    "!pip install git+https://github.com/Libric0/xLSTM-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T10:31:08.853517Z",
     "iopub.status.busy": "2024-06-04T10:31:08.853114Z",
     "iopub.status.idle": "2024-06-04T10:31:12.312350Z",
     "shell.execute_reply": "2024-06-04T10:31:12.311565Z",
     "shell.execute_reply.started": "2024-06-04T10:31:08.853479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fredi/miniforge3/envs/research/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import *\n",
    "#from fastkan import FastKANLayer\n",
    "#from mamba import *\n",
    "from xLSTM.xLSTM import xLSTM as xlstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T10:31:18.194033Z",
     "iopub.status.busy": "2024-06-04T10:31:18.193572Z",
     "iopub.status.idle": "2024-06-04T10:31:19.532403Z",
     "shell.execute_reply": "2024-06-04T10:31:19.531484Z",
     "shell.execute_reply.started": "2024-06-04T10:31:18.194006Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b7\u001b[1A\u001b[1G\u001b[27G[Files: 0  Bytes: 0  [0 B/s] Re]\u001b8\u001b7\u001b[2A\u001b[1G\u001b[27G[https://raw.githubusercontent.]\u001b8\u001b7\u001b[1S\u001b[3A\u001b[1G\u001b[0JSaving 'input.txt.1'\n",
      "\u001b8\u001b7\u001b[2A\u001b[1Ginput.txt.1           22% [=====>                        ]   95.06K    --.-KB/s\u001b8\u001b7\u001b[1A\u001b[1G\u001b[27G[Files: 0  Bytes: 0  [0 B/s] Re]\u001b8\u001b7\u001b[2A\u001b[1Ginput.txt.1          100% [=============================>]  424.87K    1.57MB/s\u001b8\u001b7\u001b[1S\u001b[3A\u001b[1G\u001b[0JHTTP response 200  [https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt]\n",
      "\u001b8\u001b7\u001b[2A\u001b[1Ginput.txt.1          100% [=============================>]  424.87K    1.57MB/s\u001b8\u001b7\u001b[1A\u001b[1G\u001b[27G[Files: 1  Bytes: 424.87K [352.]\u001b8\u001b[m\u001b[m\u001b[m\u001b[m1115394\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "  with open('/kaggle/working/input.txt', 'r', encoding='utf-8') as f:\n",
    "      text = f.read()\n",
    "except:\n",
    "  !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "  with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "      text = f.read()\n",
    "print(len(text))\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.1*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[n:]\n",
    "val_data = data[:n]\n",
    "\n",
    "\"\"\"\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\"\"\"\n",
    "\n",
    "# data loading\n",
    "def get_batch(split, block_size, batch_size):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "def get_batch_xlstm(split, block_size, batch_size, idx):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    if idx+block_size+batch_size > len(data):\n",
    "        idx = 1\n",
    "    ix = []\n",
    "    [ix.append(i) for i in range(idx, idx+batch_size)]\n",
    "    ix = torch.tensor(ix)\n",
    "    \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T10:31:39.155100Z",
     "iopub.status.busy": "2024-06-04T10:31:39.154608Z",
     "iopub.status.idle": "2024-06-04T10:31:39.188369Z",
     "shell.execute_reply": "2024-06-04T10:31:39.187417Z",
     "shell.execute_reply.started": "2024-06-04T10:31:39.155057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "  \"\"\"one head of self-attention\"\"\"\n",
    "\n",
    "  def __init__(self, head_size):\n",
    "      super().__init__()\n",
    "      self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "      self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "      self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "      self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B,T,C = x.shape\n",
    "    k = self.key(x)\n",
    "    q = self.query(x)\n",
    "    # compute attention score (\"affinities\")\n",
    "    wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "    wei = F.softmax(wei, dim=-1)# (B, T, T)\n",
    "    wei = self.dropout(wei)\n",
    "    # perform the weighted aggregation of the values\n",
    "    v = self.value(x)\n",
    "    out = wei @ v # (B, T, C)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "  def __init__(self, num_heads, head_size):\n",
    "      super().__init__()\n",
    "      self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "      self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "    out = self.dropout(self.proj(out))\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "  def __init__(self, n_embd):\n",
    "    super().__init__()\n",
    "    if n_embd % 2 != 0:\n",
    "        print(\"error the embd must be divisible by 2\")\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Linear(n_embd, 4 * n_embd),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4 * n_embd, n_embd),\n",
    "        nn.Dropout(dropout),\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    x = self.net(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "  \"\"\" Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "  def __init__(self, n_embd, n_head):\n",
    "     super().__init__()\n",
    "     head_size = n_embd // n_head\n",
    "     self.sa = MultiHeadAttention(n_head, head_size)\n",
    "     self.ffwd = FeedForward(n_embd)\n",
    "     self.ln1 = nn.LayerNorm(n_embd)\n",
    "     self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.sa(self.ln1(x))\n",
    "    x = x + self.ffwd(self.ln2(x))\n",
    "    return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, x, config_layers, device):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_embd = x.shape[2]\n",
    "        self.block_size = x.shape[1]\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(self.vocab_size, self.n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, self.n_embd)\n",
    "        \n",
    "        self.xlstm = xlstm(config_layers, x)\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(self.n_embd)\n",
    "        self.head = nn.Linear(self.n_embd, self.vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=self.device)) # T, C\n",
    "\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "\n",
    "        x = self.xlstm(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last self.block_size tokens\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx, idx_next\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters, block_size, batch_size):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, block_size, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "import random\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss_xlstm(model, eval_iters, block_size, batch_size):\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        idx = random.randint(0, 100000) if split == 'val' else random.randint(0, 1000000)\n",
    "        for k in range(eval_iters):\n",
    "            #X, Y = get_batch_xlstm(split, block_size, batch_size, idx)\n",
    "            X, Y = get_batch(split, block_size, batch_size)\n",
    "            idx += 1     \n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T10:31:39.777480Z",
     "iopub.status.busy": "2024-06-04T10:31:39.777117Z",
     "iopub.status.idle": "2024-06-04T10:31:40.013758Z",
     "shell.execute_reply": "2024-06-04T10:31:40.012804Z",
     "shell.execute_reply.started": "2024-06-04T10:31:39.777451Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "2 686 549 "
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 24 # how many independent sequences will we process in parallel?\n",
    "block_size = 124 # what is the maximum context length for predictions? # impact little\n",
    "eval_iters = 3 # more fast ( when it's low )\n",
    "device = 'cpu'\n",
    "print(device)\n",
    "n_embd = 164 # impact big \n",
    "\n",
    "dropout = 0.2 # no impact\n",
    "config_block = \"m\"\n",
    "num_heads = 8    # define number of block diagonal\n",
    "head_size = 4    # define hiddden size\n",
    "# ------------\n",
    "\n",
    "torch.set_default_device(device)\n",
    "\n",
    "x = torch.zeros(batch_size, block_size, n_embd)\n",
    "\n",
    "model = Transformer(vocab_size, x, config_block, device)\n",
    "m = model.to(device)\n",
    "\n",
    "paras = list(str(sum(p.numel() for p in m.parameters())))\n",
    "num = len(paras)-1\n",
    "for i in paras:\n",
    "  if num % 3 == 0:\n",
    "    print(i, end=\" \")\n",
    "    pass\n",
    "  else:\n",
    "    print(i, end=\"\")\n",
    "    pass\n",
    "  num -= 1\n",
    "\n",
    "# vbasic 5 000 073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T10:38:13.535696Z",
     "iopub.status.busy": "2024-06-04T10:38:13.535336Z",
     "iopub.status.idle": "2024-06-04T10:42:29.487624Z",
     "shell.execute_reply": "2024-06-04T10:42:29.486730Z",
     "shell.execute_reply.started": "2024-06-04T10:38:13.535667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not the same model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3333 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003\n",
      "step 0: train loss 4.3787, val loss 4.3765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 100/3333 [00:29<13:00,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003\n",
      "step 100: train loss 2.3726, val loss 2.4244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 141/3333 [00:39<15:03,  3.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m     plt.ylabel(\u001b[33m\"\u001b[39m\u001b[33mLoss\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     69\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(m)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# evaluate the loss\u001b[39;00m\n\u001b[32m     49\u001b[39m start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m logits, loss = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m end = time.time()\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m#print(\"exe time : \", (end-start)/batch_size)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, idx, targets)\u001b[39m\n\u001b[32m    112\u001b[39m pos_emb = \u001b[38;5;28mself\u001b[39m.position_embedding_table(torch.arange(T, device=\u001b[38;5;28mself\u001b[39m.device)) \u001b[38;5;66;03m# T, C\u001b[39;00m\n\u001b[32m    114\u001b[39m x = tok_emb + pos_emb \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_f(x)\n\u001b[32m    120\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.head(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/xLSTM/xLSTM.py:24\u001b[39m, in \u001b[36mxLSTM.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     22\u001b[39m x_original = x.clone()\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m      x = \u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m + x_original\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/xLSTM/mLSTMblock.py:87\u001b[39m, in \u001b[36mmLSTMblock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# end mLSTM\u001b[39;00m\n\u001b[32m     85\u001b[39m ht = ht\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m left = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdrop2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mGN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mht\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_skip\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m out = \u001b[38;5;28mself\u001b[39m.ln_out(left * right)\n\u001b[32m     90\u001b[39m out = \u001b[38;5;28mself\u001b[39m.ln_proj(\u001b[38;5;28mself\u001b[39m.proj(out))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/nn/functional.py:1419\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1407\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"During training, randomly zeroes some elements of the input tensor with probability :attr:`p`.\u001b[39;00m\n\u001b[32m   1408\u001b[39m \n\u001b[32m   1409\u001b[39m \u001b[33;03mUses samples from a Bernoulli distribution.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1416\u001b[39m \u001b[33;03m    inplace: If set to ``True``, will do this operation in-place. Default: ``False``\u001b[39;00m\n\u001b[32m   1417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1420\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\n\u001b[32m   1421\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/overrides.py:1721\u001b[39m, in \u001b[36mhandle_torch_function\u001b[39m\u001b[34m(public_api, relevant_args, *args, **kwargs)\u001b[39m\n\u001b[32m   1717\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[32m   1718\u001b[39m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[32m   1719\u001b[39m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[32m   1720\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[32m-> \u001b[39m\u001b[32m1721\u001b[39m         result = \u001b[43mmode\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1722\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   1723\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/utils/_device.py:104\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    103\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/research/lib/python3.12/site-packages/torch/nn/functional.py:1425\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1423\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1426\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = Transformer(vocab_size, x, config_block, device)\n",
    "try:\n",
    "    model.load_state_dict(torch.load('/kaggle/working/model'))\n",
    "    print(\"model loaded\")\n",
    "except:\n",
    "    print(\"not the same model\")\n",
    "\n",
    "m = model.to(device)\n",
    "m.train()\n",
    "\n",
    "\n",
    "def train(m):\n",
    "    learning_rate = 3e-4\n",
    "    optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 300, eta_min=1e-5)\n",
    "\n",
    "\n",
    "    loss_list_t = []\n",
    "    loss_list_v = []\n",
    "    max_iters_total = 80000\n",
    "    max_iters = int(max_iters_total/batch_size)\n",
    "    eval_interval = 100\n",
    "\n",
    "    idx = 0\n",
    "    chunk_size = 25\n",
    "\n",
    "    for iter in tqdm(range(max_iters)):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "\n",
    "        if iter % eval_interval == 0:\n",
    "            torch.save(m.state_dict(), '/kaggle/working/model')\n",
    "\n",
    "            print(optimizer.param_groups[0][\"lr\"])\n",
    "            losses = estimate_loss(m, eval_iters, block_size, batch_size)\n",
    "            loss_list_t.append(losses['train'].cpu())\n",
    "            loss_list_v.append(losses['val'].cpu())\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        if iter % chunk_size == 0:\n",
    "            idx = random.randint(0, 1000000)\n",
    "\n",
    "        # sample a batch of data\n",
    "        #xb, yb = get_batch_xlstm('train', block_size, batch_size, idx)\n",
    "        xb, yb = get_batch('train', block_size, batch_size)\n",
    "        idx += 1\n",
    "\n",
    "        # evaluate the loss\n",
    "        start = time.time()\n",
    "        logits, loss = m(xb, yb)\n",
    "        end = time.time()\n",
    "        #print(\"exe time : \", (end-start)/batch_size)\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(m.parameters(), max_norm=0.5)\n",
    "\n",
    "        #scheduler.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    torch.save(m.state_dict(), '/kaggle/working/model')\n",
    "    # draw loss\n",
    "    print(loss)\n",
    "    plt.plot(range(len(loss_list_t)), loss_list_t)\n",
    "    plt.plot(range(len(loss_list_v)), loss_list_v)\n",
    "    plt.xlabel(\"Number of Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "train(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T10:50:05.988684Z",
     "iopub.status.busy": "2024-06-04T10:50:05.987932Z",
     "iopub.status.idle": "2024-06-04T10:50:09.313467Z",
     "shell.execute_reply": "2024-06-04T10:50:09.312488Z",
     "shell.execute_reply.started": "2024-06-04T10:50:05.988647Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ourry cowardom leave my daubt. Stide,\n",
      "Shake thee arm\n",
      "Halpier of withatly. But what part's with wo\n",
      "rave reportself torlight: the He showes to them\n",
      "I'll finectefce my truth,\n",
      "That kin oursed by tone weake, sir, adval: chase. Go, blassan, got hereoms. Becaust.--\n",
      "Sarth allour lorthess obellave tumple to coow;\n",
      "Away, bried enturend Exerlar wif tray--\n",
      "With, from whis, livial pless'd himbly fet.\n",
      "\n",
      "KATHAlfied? Richmeremys k'te Mont thuncizaus\n",
      "For en arites times if I have beat\n",
      "No withough by him:\n",
      "Some of w"
     ]
    }
   ],
   "source": [
    "model = Transformer(vocab_size, x, config_block, device)\n",
    "model.load_state_dict(torch.load('/kaggle/working/model'))\n",
    "model = model.to(device)\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "#context = torch.tensor(encode(\"\"), dtype=torch.long, device=device)\n",
    "#context = context.view(1, context.shape[0])\n",
    "\n",
    "for _ in range(500):\n",
    "    context, out = model.generate(context, max_new_tokens=1)\n",
    "    print(decode(out[0].tolist()), end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30716,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
